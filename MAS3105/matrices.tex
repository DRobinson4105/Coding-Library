\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Matrix and Linear Algebra}
\author{David Robinson}
\date{}
\setlength{\parindent}{0pt}

\begin{document}
\maketitle

\section*{Linear Equations in Linear Algebra}

A matrix is in \textbf{echelon form} if:
\begin{enumerate}
    \item All nonzero rows are above any rows of all zeros.
    \item Each leading entry of a row is in a column to the right of the leading entry of the row
    above it.
    \item All entries in a column below a leading entry are zeros.
\end{enumerate}

\noindent
A matrix is in \textbf{reduced echelon form} if:
\begin{enumerate}
    \item It is in echelon form.
    \item The leading entry in each nonzero row is 1.
\end{enumerate}

\subsection*{Properties}
\begin{itemize}
    \item Two matrices are row equivalent if there exists a sequence of elementary row operations
    that transforms one matrix into the other.
    \item Each matrix is row equivalent to only one reduced echelon matrix.
    \item The echelon form of a matrix is not unique, but the reduced echelon form is unique.
\end{itemize}

\section*{Existence and Uniqueness Theorem}
A linear system is consistent if the rightmost column of echelon form of the augmented matrix is
not a pivot column.

\section*{Row Reduction Algorithm}
The row reduction algorithm leads directly to an explicit description of the solution set of a
linear system when the algorithm is applied to the augmented matrix of the system, leading to a
general solution of a system.
\begin{enumerate}
    \item Forward Phase (reducing a matrix to echelon form)
    \begin{enumerate}
        \item Begin with the leftmost nonzero column. This is a pivot column. The pivot position is
        at the top.
        \item Select a nonzero entry in the pivot column as a pivot. If necessary, interchange rows
        to move this entry into the pivot position.
        \item Use row replacement operations to create zeros in all positions below the pivot.
        \item Ignore the row containing the pivot position and all rows above it.
        \item Repeat until there are no more nonzero rows to modify.
    \end{enumerate}
    \item Backward Phase (reducing a matrix to reduced echelon form)
    \begin{enumerate}
        \item Beginning with the rightmost pivot and working upward and to the left, create zeros
        above each pivot. If a pivot is not 1, make it 1 by a scaling operation.
    \end{enumerate}
\end{enumerate}

\section*{Span (Linear Combination)}
\begin{itemize}
    \item The span of two vectors, $\text{Span}\{\mathbf{v}_1, \mathbf{v}_2\}$, represents all
    vectors that can be reached by scaling and adding the two vectors.
    \item If the system consisting of vectors $\mathbf{v}_1$, $\mathbf{v}_2$, and $\mathbf{b}$ is
    consistent, then $\mathbf{b}$ is in $\text{Span}\{\mathbf{v}_1, \mathbf{v}_2\}$.
    \item A matrix can only span $\mathbb{R}^n$ if it has pivot positions in $n$ rows.
\end{itemize}

\section*{Matrix Equation $A\mathbf{x}=b$}
$A\mathbf{x}=b$ can be represented as a vector or matrix equation.
\[\begin{split}
    ax_1+bx_2+cx_3=d \\
    ex_1+fx_2+gx_3=h
\end{split}\]

Vector Equation:
\[x_1\begin{bmatrix} a \\ e \end{bmatrix} + x_2\begin{bmatrix} b \\ f \end{bmatrix} + x_3
\begin{bmatrix} c \\ h \end{bmatrix} = \begin{bmatrix} d \\ h \end{bmatrix}\]
Matrix Equation:
\[\begin{bmatrix} a & b & c \\ e & f & g \end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} d \\ h \end{bmatrix}\]
Augmented Matrix:
\[\begin{bmatrix}
    a & b & c & d \\
    e & f & g & h
\end{bmatrix}\]

Let $A$ be an $m \times n$ matrix. Then the following statements are logically equivalent. That is,
for a particular $A$, either they are all true statements or they are all false.
\begin{enumerate}
    \item For each $\textbf{b}$ in $\mathbb{R}^m$, the equation $A\mathbf{x}=\mathbf{b}$ has a
    solution.
    \item Each $\textbf{b}$ in $\mathbb{R}^m$ is a linear combination of the columns of $A$.
    \item The columns of $A$ span $\mathbb{R}^m$.
    \item $A$ has a pivot position in every row.
\end{enumerate}

\section*{Homogeneous Equation}
A homogeneous equation is a linear equation in the form $Ax=0$

\begin{enumerate}
    \item The homogeneous equation always has at least one solution (the trivial solution), where
    $\textbf{x}=\textbf{0}$.
    \item The homogeneous equation has a nontrivial solution if the equation has at least one free
    variable.
    \item If the matrix $\textbf{A}$ has more columns than rows ($n > m$), the system often has
    infinitely many solutions.
    \item If $\textbf{A}$ has $n$ pivot columns, the columns of $\textbf{A}$ are linearly
    independent, since every variable is a basic variable.
\end{enumerate}

\section*{Parametric Vector Equation}
The equation can be represented in parametric vector form if there is a free variable so that all
of the other variables are represented in terms of the free variable. For example, if $x_3$ is a
free variable in $\mathbb{R}^3$, 
\[x=\begin{bmatrix} c \\ d \\ 0 \end{bmatrix} + x_3
\begin{bmatrix} a \\ b \\ 1\end{bmatrix}\] 
where $x_1=ax_3 + c$ and $x_2=bx_3 + d$.\newline

$x = \begin{bmatrix}
    a + bx_3 \\ c + dx_3 \\ e + fx_3
\end{bmatrix}$, geometrically describes a line through $\begin{bmatrix}
    a \\ c \\ e
\end{bmatrix}$ parallel to $\begin{bmatrix}
    b \\ d \\ f
\end{bmatrix}$\newline

The solution set of $A\mathbf{x}=\mathbf{b}$ is the set of all vectors of the form
$\mathbf{w}=\mathbf{p}+\mathbf{v}_h$, where $\mathbf{v}_h$ is any solution of the equation
$A\mathbf{x}=\mathbf{0}$, only when the equation $A\mathbf{x}=\mathbf{b}$ is consistent for some
given $\mathbf{b}$, and there exists a vector $\mathbf{p}$ such that $\mathbf{p}$ is a solution.

\section*{Linear Independence}
An indexed set of vectors $\{\mathbf{v}_1,\cdots,\mathbf{v}_p\}$ in $\mathbb{R}^n$ is said to be
linearly independent if the vector equation $x_1\mathbf{v}_1+\cdots + x_p\mathbf{v}_p=0$ has only
the trivial solution.
\begin{itemize}
    \item If a set $S=\{\mathbf{v}_1,\cdots, \mathbf{v}_p\}$ in $\mathbb{R}^n$ contains the zero
    vector, then the set is linearly dependent.
    \item Two vectors are linearly dependent if they live on a line through the origin.
\end{itemize}

\section*{Linear Transformations}
A linear transformation is a function from $\mathbb{R}^n$ to $\mathbb{R}^m$ that assigns to each
vector $\mathbf{x}$ in $\mathbb{R}^n$ a vector $T(\mathbf{x})$ in $\mathbb{R}^m$. For a
transformation to be linear, then:
\begin{itemize}
    \item $T(\mathbf{0})=0$.
    \item $T(c\mathbf{u}+d\mathbf{v})=cT(\mathbf{u})+dT(\mathbf{v})$
\end{itemize}

\subsection*{Key Points}
\begin{itemize}
    \item If $A$ is a $m\times n$ matrix and $T$ is a transformation defined by
    $T(\mathbf{x})=A\mathbf{x}$, then the domain of $T$ is $\mathbb{R}^n$ and the range of $T$ is
    the solution set.
    \item A mapping $T$: $\mathbb{R}^n\rightarrow\mathbb{R}^m$ is one-to-one if each vector in
    $\mathbb{R}^n$ maps onto a unqiue vector in $\mathbb{R}^m$. Also, $T$ is one-to-one if and only
    if the columns of $A$ are linearly independent.
    \item A mapping $T$: $\mathbb{R}^n\rightarrow\mathbb{R}^m$ is onto $\mathbb{R}^m$ if every
    vector in $\mathbb{R}^m$ is mapped onto by some vector $\mathbf{x}$ in $\mathbb{R}^n$. Also,
    $T$ is onto $\mathbb{R}^m$ if there are $n$ pivot columns.
\end{itemize}

Standard matrix of $T$ to rotate points about the origin by $\theta$:
$A=\begin{bmatrix}
    \cos\theta & -\sin\theta \\
    \sin\theta & \cos\theta
\end{bmatrix}$

If $T$: $\mathbb{R}^n\rightarrow\mathbb{R}^m$ is a linear transformation, then there exists a
unique matrix $A$ such that the following equation is true.
\[T(\mathbf{x})=A\mathbf{x}\text{ for all }\mathbf{x}\text{ in }\mathbb{R}^n\]
In fact, $A$ is the $m\times n$ matrix whose jth column is the vector, $T(e_j)$, where $e_j$ is the
jth column of the identity matrix in $\mathbb{R}^n$, as shown in the following equation.
\[A=\begin{bmatrix}
    T(e_1) & \cdots & T(e_n) 
\end{bmatrix}\]

\section*{Matrix Multiplication}
The definition of matrix multiplication states that if $A$ is an $m\times n$ matrix and $B$ is an
$n\times p$ matrix with columns $\mathbf{b}_1, \cdots , \mathbf{b}_p$, then
$AB=\begin{bmatrix} A\mathbf{b}_1 & \cdots & A\mathbf{b}_p \end{bmatrix}$.

\[
\begin{bmatrix}
    a & b \\
    c & d \\
    e & f
\end{bmatrix}
\times
\begin{bmatrix}
    g & h \\
    i & j
\end{bmatrix}
=
\begin{bmatrix}
    ag+bi & ah+bj \\
    cg+di & ch+dj \\
    eg+fi & eh+fj
\end{bmatrix}
\]

The $(i, j)$ entry in $AB$ is
\[\sum_{k=1}^n a_{\text{ik}}b_{\text{kj}}\quad \text{or} \quad a_{\text{i1}}b_{\text{1j}}+\cdots +
a_{\text{in}}b_{\text{nj}}\]

\[{(AB)}^T=B^T A^T\]
\subsubsection*{Key Points}
\begin{itemize}
    \item Matrix multiplication is only allowed for matrices where the number of columns in the
    first matrix equal the number of rows in the second matrix.
    \item If $A$ is a $a\times b$ matrix and $B$ is a $b \times c$ matrix, then $AB$ is a
    $a \times c$ matrix.
    \item Given a square $n\times n$ matrix $A$, there are infinite matrices for $B$ in $AB=BA$
    because any multiple of $I_n$ will satisfy the equation.
    \item The definition of $AB$ states that each column of $AB$ is a linear combination of the
    columns of A using weights from the corresponding column of $B$.
\end{itemize}

\section*{Inverse Matrices}
\[A\times A^{-1}=I\]
where $A$ is an invertible matrix, $A^{-1}$ is the inverse of $A$, and $I$ is the identity matrix
with the same shape as $A$.
Calculating the inverse of a $2\times 2$ square matrix.

\subsubsection*{Calculating the Inverse of a Matrix}
\begin{enumerate}
    \item Start with a square matrix $A$.
    \item Reduce the matrix $\begin{bmatrix} A & I \end{bmatrix}$ (where I is the identity matrix
    with the same shape as A) to reduced echelon form which will result in
    $\begin{bmatrix} I & A^{-1} \end{bmatrix}$.
\end{enumerate}

If $A$ is a $2\times 2$ matrix, then
$A^{-1}=\frac{1}{ad-bc}\begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$. \\

If $A$ is an invertible $n\times n$ matrix, then for each $\mathbf{b}$ in $\mathbb{R}^n$, the
equation $A\mathbf{x}=\mathbf{b}$ has the unique solution $\mathbf{x}=A^{-1}\mathbf{b}$.

\subsection*{Invertible Matrix Theorem}
The following statements are either all true or all false
\begin{enumerate}
    \item $A$ is an invertible matrix.
    \item $A$ has $n$ pivot positions.
    \item The columns of $A$ form a linearly independent set.
    \item The columns of $A$ span $\mathbb{R}^n$.
    \item There are $n\times n$ matrices $C$ and $D$ such that $CA=I$ and $AD=I$.
\end{enumerate}

The deteminant of a standard $n\times n$ matrix is $ad-bc$.

\subsubsection*{Key Points}
\begin{itemize}
    \item $(AB)^{-1}$ is $B^{-1}A^{-1}$.
    \item If $A$ is an invertible $n\times n$ matrix, then the equation $A\mathbf{x}=\mathbf{b}$ is
    consistent for each $\mathbf{b}$ in $\mathbb{R}^n$ because $A^{-1}\mathbf{b}$ exists for all
    $\mathbf{b}$ in $\mathbb{R}^n$ and $\mathbf{x}=A^{-1}\mathbf{b}$.
    \item The columns of an $n\times n$ matrix $A$ are linearly independent when $A$ is invertible
    because the equation $A\mathbf{x}=0$ has the unique solution $\mathbf{x}=0$.
    \item The product of invertible matrices is also invertible.
    \item If $A$ and $B$ are $n\times n$ matrices and $AB$ is invertible, then $B$ is also
    invertible
\end{itemize}

\section*{LU Factorization}
For a $3x3$ matrix, LU Factorization looks like
\[\begin{bmatrix}
    \# & \# & \# \\ \# & \# & \# \\ \# & \# & \#
\end{bmatrix}=\begin{bmatrix}
    1 & 0 & 0 \\ \# & 1 & 0 \\ \# & \# & 1
\end{bmatrix}\begin{bmatrix}
    \# & \# & \# \\ 0 & \# & \# \\ 0 & 0 & \#
\end{bmatrix}\] where \# is any number.
\begin{enumerate}
    \item Start with a matrix $A$.
    \item Reduce $A$ to echelon form $U$.
    \item Before reducing each column for $U$, place that column with 0's above the current pivot
    position into $L$.
    \item At each pivot column in $L$, divide the column by the pivot point
\end{enumerate}

If $A$ is a $n\times m$ matrix, $L$ is a $n\times n$ matrix and $U$ is a $n\times m$ matrix.

\section*{Other}
\begin{enumerate}
    \item It is not possible that $CA=I_4$ for some $4\times 2$ matrix $C$ when $A$ is a
    $2\times 4$ matrix because if it were true, then $CA\mathbf{x}$ would equal $\mathbf{x}$ for
    all $\mathbf{x}$ in $\mathbb{R}^4$. Since the columns of $A$ are linearly dependent as there
    are more columns than rows, $A\mathbf{x}=0$ for some nonzero vector $\mathbf{x}$, so
    $\mathbf{x}=I\mathbf{x}\neq CA\mathbf{x}=0$.
\end{enumerate}
\end{document}
