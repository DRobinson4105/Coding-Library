\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{txfonts}

\title{Chapter 5 Eigenvalues and Eigenvectors}
\author{David Robinson}
\date{}
\setlength{\parindent}{0pt}

\begin{document}
\maketitle

\section*{Eigenvalues and Eigenvectors}

An \textbf{eigenvector} of an $n\times n$ matrix $A$ is a nonzero vector $\mathbf{x}$ such that
$A\mathbf{x}=\lambda\mathbf{x}$ for some scalar $\lambda$. A scalar $\lambda$ is called an
\textbf{eigenvalue} of $A$ if there is a nontrivial solution $\mathbf{x}$ of
$A\mathbf{x}=\lambda\mathbf{x}$; such an $\mathbf{x}$ is called an
\textit{eigenvector corresponding to $\lambda$.}

\subsubsection*{Theorem 1}
The eigenvalues of a triangular matrix are the entries on its main diagonal.

\subsubsection*{Theorem 2}
If $\mathbf{v}_1, \ldots, \mathbf{v}_r$ are eigenvectors that correspond to distinct eigenvalues
$\lambda_1, \ldots, \lambda_r$ of an $n\times n$ matrix $A$, then the set
$\{\mathbf{v}_1, \ldots, \mathbf{v}_r\}$ is linearly independent.

\subsection*{Validating an Eigenvalue}
\begin{enumerate}
    \item Start with the equation $A\mathbf{x}=\lambda \mathbf{x}$
    \item Form the matrix $A-\lambda I$
    \item If the columns are linearly dependent, $\lambda$ is an eigenvalue
    \item Reduce the matrix to reduced echelon form and each column vector in terms of the free
    variables is a corresponding eigenvector and a part of the basis for the eigenspace
\end{enumerate}

\subsection*{Validating an Eigenvector}
\begin{enumerate}
    \item Start with the equation $A\mathbf{x}=\lambda\mathbf{x}$
    \item Compute the product of $A\mathbf{x}$
    \item If $A\mathbf{x}$ is proportional to $\mathbf{x}$, then $\mathbf{x}$ is an eigenvector and
    the scaling factor is the eigenvalue
\end{enumerate}

\subsection*{Key Points}
\begin{itemize}
    \item If the columns of $A$ are linearly dependent, one eigenvalue of $A$ is $\lambda=0$
    \item If $A$ is the zero matrix, then the only eigenvalue of $A$ is $0$
\end{itemize}

\section*{The Characteristic Equation}
The scalar equation $\det (A-\lambda I) = 0$ is called the characteristic equation. The
characteristic polynomial is the simplified polynomial in the characteristic equation and the
eigenvalues are the values for $\lambda$.
\subsubsection*{Theorem 3 --- Properties of Determinants}
Let $A$ and $B$ be $n\times n$ matrices.
\begin{enumerate}
    \item $A$ is invertible if and only if $\det A \neq 0$.
    \item $\det AB=(\det A)(\det B)$.
    \item $\det A^T = \det A$.
    \item If $A$ is triangular, then $\det A$ is the product of the entries on the main diagonal of
    $A$.
    \item A row replacement operation on $A$ does not change the determinant. A row interchange
    changes the sign of the determinant. A row scaling also scales the determinant by the same
    scalar factor.
\end{enumerate}

\subsubsection*{The Invertible Matrix Theorem (continued)}
Let $A$ be an $n\times n$ matrix. Then $A$ is invertible if and only if, the number $0$ is
\textbf{not} an eigenvalue of $A$.

\subsubsection*{Theorem 4}
If $n\times n$ matrices $A$ and $B$ are similar, then they have the same characteristic polynomial
and hence the same eigenvalues (with the same multiplicities).

\pagebreak

\section*{Diagonalization}
\subsubsection*{Theorem 5 --- The Diagonalization Theorem}
An $n\times n$ matrix $A$ is diagonalizable if and only if $A$ has $n$ linearly independent
eigenvectors.

In fact, $A=PDP^{-1}$, with $D$ a diagonal matrix, if and only if the columns of $P$ are $n$
linearly independent eigenvectors of $A$. In this case, the diagonal entries of $D$ are eigenvalues
of $A$ that correspond, respectively, to the eigenvectors in $P$.

\vspace{1em}
\textbf{In other words, $A$ is diagonalizable if and only if there are enough eigenvectors to form
a basis of $\mathbb{R}^n$, known as an eigenvector basis of $\mathbb{R}^n$.}

\subsubsection*{Theorem 6}
An $n\times n$ matrix with $n$ distinct eigenvalues is diagonalizable.

\subsubsection*{Theorem 7}
Let $A$ be an $n\times n$ matrix whose distinct eigenvalues are $\lambda_1, \ldots, \lambda_p$.
\begin{enumerate}
    \item For $1 \leq k \leq p$, the dimension of the eigenspace for $\lambda_k$ is less than or
    equal to the multiplicity of the eigenvalue $\lambda_k$.
    \item The matrix $A$ is diagonalizable if and only if the sum of the dimensions of the
    eigenspaces equals $n$, and this happens if and only if the characteristic polynomial factors
    completely into linear factors and the dimension of the eigenspace for each $\lambda_k$ equals
    the multiplicity of $\lambda_k$.
    \item If $A$ is diagonalizable and $\mathcal{B}_k$ is a basis for the eigenspace corresponding
    to $\lambda_k$ for each $k$, then the total collection of vectors in the sets
    $\mathcal{B}_1, \ldots, \mathcal{B}_p$ forms an eigenvector basis for $\mathbb{R}^n$.
\end{enumerate}

\subsection*{Key Points}
\begin{itemize}
    \item $A=PDP^{-1}, A^2 = PD^2 P^{-1}, \ldots, A^k=PD^k P^{-1}$
    \item If $A$ is both diagonalizable and invertible, then so is $A^{-1}$.
        \begin{enumerate}
            \item If $A$ is diagonalizable, then $A=PDP^{-1}$ for some invertible $P$ and diagonal
            $D$.
            \item Zero is not an eigenvalue of $A$, so the diagonal entries in $D$ are not zero, so
            $D$ is invertible.
            \item $A^{-1}=PD^{-1}P^{-1}$, therefore $A^{-1}$ is also diagonalizable.
        \end{enumerate}
\end{itemize}
\end{document}