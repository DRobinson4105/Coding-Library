\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{txfonts}

\title{Chapter 6 Orthogonality and Least Squares}
\author{David Robinson}
\date{}
\setlength{\parindent}{0pt}

\begin{document}
\maketitle

\section*{Inner Product, Length, and Orthogonality}

\subsubsection*{Theorem 1}
Let $\mathbf{u}$, $\mathbf{v}$, and $\mathbf{w}$ be vectors in $\mathbb{R}^n$, and let $c$ be a
scalar. Then
\begin{enumerate}
    \item $\mathbf{u}\cdot\mathbf{v}=\mathbf{v}\cdot\mathbf{u}$
    \item $(\mathbf{u}+\mathbf{v})\cdot\mathbf{w}=\mathbf{u}\cdot\mathbf{w}+\mathbf{v}\cdot
    \mathbf{w}$
    \item $(c\mathbf{u})\cdot\mathbf{v}=c(\mathbf{u}\cdot\mathbf{v})=u\cdot(c\mathbf{v})$
    \item $\mathbf{u}\cdot\mathbf{u}\geq 0$, and $\mathbf{u}\cdot\mathbf{u}=0$ if and only if
    $\mathbf{u}=0$
\end{enumerate}

\subsubsection*{The Length of a Vector}
The length (or norm) of $\mathbf{v}$ is the nonnegative scalar $\|\mathbf{v}\|$ defined by
\[\|\mathbf{v}\|=\sqrt{\mathbf{v}\cdot\mathbf{v}}=\sqrt{v_1^2+v_2^2+\cdots+v_n^2}\text{,\quad and
\quad}\|\mathbf{v}\|^2=\mathbf{v}\cdot\mathbf{v}\]

\subsubsection*{Distance in $\mathbb{R}^n$}
For $\mathbf{u}$ and $\mathbf{v}$ in $\mathbb{R}^n$, the \textbf{distance between u and v}, written
as dist$(\mathbf{u}, \mathbf{v})$, is the length of the vector $\mathbf{u}-\mathbf{v}$.
\[\text{dist}(\mathbf{u}, \mathbf{v})=\|\:\mathbf{u}-\mathbf{v}\:\|\]

\subsubsection*{Orthogonal Vectors}
Two vectors $\mathbf{u}$ and $\mathbf{v}$ in $\mathbb{R}^n$ are \textbf{orthogonal} (to each other)
if $\mathbf{u}\cdot\mathbf{v}=0$.

\subsubsection*{Theorem 2 --- The Pythagorean Theorem}
Two vectors $\mathbf{u}$ and $\mathbf{v}$ are orthogonal if and only if
$\|\:\mathbf{u}+\mathbf{v}\:\|^2 = \|\mathbf{u}\|^2 + \|\mathbf{v}\|^2$.

\subsubsection*{Theorem 3}
Let $A$ be an $m\times n$ matrix. The orthogonal complement of the row space of $A$ is the null
space of $A$, and the orthogonal complement of the column space of $A$ is the null space of $A^T$
\[{(\text{Row }A)}^\perp = \text{Nul }A\quad\text{and}\quad{(\text{Col }A)}^\perp=\text{Nul }A^T\]

\subsection*{Key Points}
\begin{enumerate}
    \item $\mathbf{u}\cdot\mathbf{v}=\mathbf{u}^T\times \mathbf{v}$
    \item A unit vector in the direction of a vector can be determined by dividing that vector by
    its length.
    \item $\|c\mathbf{v}\|$ is not always equal to $c\|\mathbf{v}\|$. Since lenght is always
    positive, the value of $\|c\mathbf{v}\|$ is positive for all values of c. However,
    $c\|\mathbf{v}\|$ is negative if $c$ is negative.
\end{enumerate}

\section*{Orthogonal Sets}

\subsubsection*{Theorem 4}
If $S=\{\mathbf{u}_1, \ldots, \mathbf{u}_p\}$ is an orthogonal set of nonzero vectors in
$\mathbb{R}^n$, then $S$ is linearly independent and hence is a basis for the subspace spanned by
$S$.

\subsubsection*{Orthogonal Basis}
An \textbf{orthogonal basis} for a subspace $W$ of $\mathbb{R}^n$ is a basis for $W$ that is also
an orthogonal set.

\subsubsection*{Theorem 5}
Let $\{\mathbf{u}_1, \ldots, \mathbf{u}_p\}$ be an orthogonal basis for a subspace $W$ of
$\mathbb{R}^n$. For each $\mathbf{y}$ in $W$, the weights in the linear combination are
\[\mathbf{y}=c_1\mathbf{u}_1 + \cdots + c_p\mathbf{c}_p\text{\quad given by \quad} c_j=
\frac{\mathbf{y}\cdot\mathbf{u}_j}{\mathbf{u}_j\cdot \mathbf{u}_j}\quad (j=1, \ldots, p)\]

\subsubsection*{Theorem 6}
An $m\times n$ matrix $U$ has orthonormal columns if and only if $U^T U=I$.

\subsubsection*{Theorem 7}
Let $U$ be an $m\times n$ matrix with orthonormal columns, and let $\mathbf{x}$ and $\mathbf{y}$ be
in $\mathbb{R}^n$. Then
\begin{enumerate}
    \item $\|U\mathbf{x}\|=\|\mathbf{x}\|$
    \item $(U\mathbf{x})\cdot(U\mathbf{y})=\mathbf{x}\cdot\mathbf{y}$
    \item $(U\mathbf{x})\cdot(U\mathbf{y})=0$ if and only if $\mathbf{x}\cdot\mathbf{y}=0$
\end{enumerate}

\subsection*{Key Points}
\begin{enumerate}
    \item A set of vectors is orthogonal if each pair of distinct vectors from the set is
    orthogonal.
    \item The vector $\mathbf{\hat{y}}$ is the orthogonal projection of $\mathbf{y}$ onto
    $\mathbf{u}$.
    \[\mathbf{\hat{y}}=\Bigg(\frac{\mathbf{y\cdot u}}{\mathbf{u\cdot u}}\Bigg)\mathbf{u}\]
    \item $\mathbf{y}$ can be writtena s the sum of a vector in $\text{Span}\{\mathbf{u}\}$ and a
    vector orthogonal to $\mathbf{u}$.
    \[\mathbf{y}=\mathbf{\hat{y}}+\mathbf{z}\]
    \item An orthonormal set is an orthogonal set where all of the vectors are unit vectors.
    \item If $A$ is a matrix with orthonormal columns, then $\|A\mathbf{x}\|=\|\mathbf{x}\|$.
    \item If $U$ is an orthogonal matrix, $U^T=U^{-1}$.
\end{enumerate}
\end{document}